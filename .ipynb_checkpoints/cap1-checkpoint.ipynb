{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ab16bb-653d-4c06-bf85-9307a94b23f8",
   "metadata": {},
   "source": [
    "# Capstone 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1178e7-ba3d-443f-8c25-cb4b5dee8f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as du\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from einops import rearrange\n",
    "\n",
    "import sidechainnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33ae80-32fb-4b10-8722-c4dd85b058a7",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9a6aeb-bfe4-44ff-9962-dabdaf2ca77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHSA(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_m, c_z, heads=8, dim_head=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.dim_head = (int(c_m / heads)) if dim_head is None else dim_head\n",
    "        _dim = self.dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.to_qvk = nn.Linear(c_m, _dim * 4, bias=False)\n",
    "        self.W_0 = nn.Linear( _dim, c_m, bias=False)\n",
    "        self.scale_factor = self.dim_head ** -0.5\n",
    "        \n",
    "        self.fc_scale_bias = nn.Linear(c_z, heads)\n",
    "\n",
    "    def forward(self, x, bias_rep=None, mask=None):\n",
    "        '''\n",
    "        x = MSA\n",
    "        bias_rep = Pair bias\n",
    "        '''\n",
    "        \n",
    "        # Step 1\n",
    "        qkv = self.to_qvk(x)  # [batch, tokens, c_m*3*heads ]\n",
    "\n",
    "        # Step 2\n",
    "        # decomposition to q,v,k and cast to tuple\n",
    "        # the resulted shape before casting to tuple will be:\n",
    "        # [3, batch, heads, tokens, dim_head]\n",
    "        q, k, v, g = tuple(rearrange(qkv, 'b t (d k h) -> k b h t d ', k=4, h=self.heads))\n",
    "        \n",
    "        # Step 3\n",
    "        # resulted shape will be: [batch, heads, tokens, tokens]\n",
    "        scaled_dot_prod = torch.einsum('b h i d , b h j d -> b h i j', q, k) * self.scale_factor\n",
    "\n",
    "        if mask is not None:\n",
    "            assert mask.shape == scaled_dot_prod.shape[2:]\n",
    "            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)\n",
    "\n",
    "        # pair wise bias\n",
    "        scaled_bias = 0\n",
    "        if self.bias:\n",
    "            scaled_bias = self.fc_scale_bias(bias_rep)\n",
    "            scaled_bias = rearrange(scaled_bias, 'i j k -> k i j').unsqueeze(0)\n",
    "            \n",
    "        attention = torch.softmax(scaled_dot_prod + scaled_bias, dim=-1)\n",
    "        \n",
    "        # Step 4. Calc result per batch and per head h\n",
    "        out = torch.einsum('b h i j , b h j d -> b h i d', attention, v)\n",
    "        \n",
    "        # gating\n",
    "        g = torch.sigmoid(g)\n",
    "        out *= g\n",
    "\n",
    "        # Step 5. Re-compose: merge heads with dim_head d\n",
    "        out = rearrange(out, \"b h t d -> b t (h d)\")\n",
    "\n",
    "        # Step 6. Apply final linear transformation layer\n",
    "        return self.W_0(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "987beea7-84c6-443a-b582-acb7169f88bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA_Stack(nn.Module):\n",
    "    def __init__(self, batch_size, c_m, c_z, heads=8, dim_head=None):\n",
    "        super().__init__()\n",
    "        # batches of row wise MHSA\n",
    "        self.row_MHSA = nn.ModuleList([MHSA(c_m=c_m, c_z=c_z, heads=heads, bias=True, dim_head=None) for i in range(batch_size)])\n",
    "        # batches of col wise MHSA\n",
    "        self.col_MHSA = nn.ModuleList([MHSA(c_m=c_m, c_z=c_z, heads=heads, bias=False, dim_head=None) for i in range(batch_size)])\n",
    "        # transition MLP\n",
    "        self.fc1 = nn.Linear(c_m, 4 * c_m)\n",
    "        self.fc2 = nn.Linear(4 * c_m, c_m)\n",
    "        \n",
    "    def forward(self, x, bias_rep):\n",
    "        res = torch.empty(x.shape)\n",
    "        # row wise gated self-attention with pair bias\n",
    "        for i, mhsa in enumerate(self.row_MHSA):\n",
    "            res[i] = mhsa(x[i], bias_rep[i])\n",
    "        x += res # add residuals\n",
    "        \n",
    "        # column wise gated self-attention\n",
    "        x_trans = rearrange(x, 'b i j k -> b j i k')\n",
    "        for i, mhsa in enumerate(self.col_MHSA):\n",
    "            res[i] = rearrange(mhsa(x_trans[i]), 'i j k -> j i k')\n",
    "        x += res # add residuals\n",
    "        \n",
    "        # transiion\n",
    "        r = F.relu(self.fc1(x))\n",
    "        r = self.fc2(r) + x\n",
    "        \n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d390ce59-56ed-433a-9d34-1973db99d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pair_Stack(nn.Module):\n",
    "    def __init__(self, batch_size, c_z, heads=8, dim_head=None):\n",
    "        super().__init__()\n",
    "        # batches of row wise MHSA\n",
    "        self.start_MHSA = nn.ModuleList([MHSA(c_m=c_z, c_z=c_z, heads=heads, bias=True, dim_head=dim_head) for i in range(batch_size)])\n",
    "        # batches of col wise MHSA\n",
    "        self.end_MHSA = nn.ModuleList([MHSA(c_m=c_z, c_z=c_z, heads=heads, bias=True, dim_head=dim_head) for i in range(batch_size)])\n",
    "        # transition MLP\n",
    "        self.fc1 = nn.Linear(c_z, 4 * c_z)\n",
    "        self.fc2 = nn.Linear(4 * c_z, c_z)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = torch.empty(x.shape)\n",
    "        # row wise gated self-attention with pair bias\n",
    "        for i, mhsa in enumerate(self.start_MHSA):\n",
    "            res[i] = mhsa(x[i], x[i])\n",
    "        x += res # add residuals\n",
    "        \n",
    "        # column wise gated self-attention\n",
    "        x_trans = rearrange(x, 'b i j k -> b j i k')\n",
    "        for i, mhsa in enumerate(self.end_MHSA):\n",
    "            # res[i] = mhsa(x_trans[i], x_trans[i])\n",
    "            res[i] = rearrange(mhsa(x_trans[i], x_trans[i]), 'i j k -> j i k')\n",
    "        x += res # add residuals\n",
    "        \n",
    "        # transiion\n",
    "        r = F.relu(self.fc1(x))\n",
    "        r = self.fc2(r) + x\n",
    "        \n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb2dbf6-cb5a-4f94-88e6-3f211c827484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Outer_Product_Mean(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_m, c_z, c=32):\n",
    "        super().__init__()\n",
    "        # linear projections\n",
    "        self.fc1 = nn.Linear(c_m, c)\n",
    "        self.fc2 = nn.Linear(c**2, c_z)\n",
    "        self.flatten = nn.Flatten(start_dim=3)\n",
    "        self.c = c\n",
    "        self.c_z = c_z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: B x S x R x C\n",
    "        res: B x R x R x C\n",
    "        '''\n",
    "        # results\n",
    "        res = torch.empty(x.shape[0], x.shape[-2], x.shape[-2], self.c, self.c)\n",
    "        \n",
    "        # project in_c to out_c\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # loop over R\n",
    "        for i in range(x.shape[-2]):\n",
    "            for j in range(x.shape[-2]):\n",
    "                mean_s = torch.mean(torch.einsum('bij,bik->bijk', [x[:, :, i, :], x[:, :, j, :]]), dim=1)\n",
    "                res[:, i, j, :, :] = mean_s\n",
    "        \n",
    "        # flatten and project back\n",
    "        res = self.flatten(res)\n",
    "        res = self.fc2(res)\n",
    "        \n",
    "        return res\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6b84672-915e-4e02-bb8b-a88908f3d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 16\n",
    "R = 256\n",
    "C_m = 256\n",
    "C_z = 128\n",
    "B = 4\n",
    "C = 16\n",
    "\n",
    "src_test = torch.rand((B, S, R, C_m))\n",
    "rrc_test = torch.rand((B, R, R, C_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ded46b-aa13-4c21-b6c2-d92db863b70a",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c04595de-b06a-48a5-95a5-5bb8477519ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 256, 256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhsa_stack = MSA_Stack(batch_size=B, c_m=C_m, c_z=C_z, heads=4, dim_head=C)\n",
    "mhsa_stack(x=src_test, bias_rep=rrc_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c71a64fe-2e37-485e-b378-a3e8dc0a0188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opm = Outer_Product_Mean(c_m=C_m, c_z=C_z, c=C)\n",
    "opm(src_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61ff9114-f9f6-4557-96d0-ef5b430383d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256, 128])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_stack = Pair_Stack(batch_size=B, c_z=C_z, heads=4, dim_head=C)\n",
    "pair_stack(x=rrc_test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539975cd-47e2-4ba0-a4af-1f646d24d51a",
   "metadata": {},
   "source": [
    "playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab5db5-3c66-41c2-af6e-326a455be932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
