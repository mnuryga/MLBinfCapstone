{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c42e0b1",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f02e8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a838a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPA(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_m, c_z, heads=12, dim_head=None, n_qp=4, n_pv=8):\n",
    "        '''\n",
    "        dim_head: channel C\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # constants\n",
    "        self.w_c = (2 / (9 * n_qp)) ** -0.5\n",
    "        self.w_l = (1 / 3) ** -0.5\n",
    "        self.n_qp = n_qp\n",
    "        self.n_pv = n_pv\n",
    "        \n",
    "        # single rep attention layers\n",
    "        self.heads = heads\n",
    "        self.dim_head = (int(c_m / heads)) if dim_head is None else dim_head\n",
    "        _dim = self.dim_head * heads\n",
    "        self.to_qvk = nn.Linear(c_m, _dim * 3, bias=False)\n",
    "        self.W_0 = nn.Linear(_dim, c_m, bias=False)\n",
    "        self.to_qk = nn.Linear(c_m, (n_qp * heads *3) * 2, bias=False)\n",
    "        self.W_0 = nn.Linear(_dim, c_m, bias=False)\n",
    "        self.W_1 = nn.Linear(heads * c_z, c_m, bias=False)\n",
    "        self.W_2 = nn.Linear(heads * n_pv * 3, c_m)\n",
    "        self.gamma = nn.Parameter(torch.rand(1))\n",
    "        self.to_v = nn.Linear(c_m, (n_pv * heads * 3), bias=False)\n",
    "        \n",
    "        # pair_rep layers\n",
    "        self.fc1 = nn.Linear(c_z, heads)\n",
    "\n",
    "    def forward(self, pair_rep, sing_rep, bbr, bbt):\n",
    "        '''\n",
    "        bbr: rotational matrix (B x R x 3 x 3)\n",
    "        bbt: translatoin matrix (B x R x 3)\n",
    "        '''\n",
    "        \n",
    "        # pair_rep to pair_bias\n",
    "        pair_bias = self.fc1(pair_rep)\n",
    "        pair_bias = rearrange(pair_bias, 'b i j h -> b h i j')\n",
    "#         print(f'pair bias shape = {pair_bias.shape}')\n",
    "        \n",
    "        ### SINGLE REP SQR ATTENTION\n",
    "        \n",
    "        # get q and v for attention training (B x P x R x H x 3)\n",
    "        qk = self.to_qk(sing_rep)\n",
    "#         print(f'qk shape = {qk.shape}')\n",
    "        gq, gk = tuple(rearrange(qk, 'b r (d k p a) -> k b p r d a', k=2, a=3, p=self.n_qp))\n",
    "#         print(f'qk shape = {gq.shape}')\n",
    "        gv = rearrange(self.to_v(sing_rep), 'b r (d p a) -> b p r d a', a=3, p=self.n_pv)\n",
    "#         print(f'gv shape = {gv.shape}')\n",
    "        \n",
    "        ### SINGLE REP DOT ATTENTION\n",
    "        \n",
    "        # get q, v, k matrices for attention training (B x H x R x C)\n",
    "        qkv = self.to_qvk(sing_rep)\n",
    "#         print(f'qkv shape = {qkv.shape}')\n",
    "        rq, rk, rv = tuple(rearrange(qkv, 'b r (d k h) -> k b h r d', k=3, h=self.heads))\n",
    "#         print(f'qkv shape = {rq.shape}')\n",
    "    \n",
    "        # dot product attention (B x H x R x R)\n",
    "        dot_prod_aff = torch.einsum('b h i d , b h j d -> b h i j', rq, rk) * (self.dim_head ** -0.5)\n",
    "#         print(f'dot_prod_aff shape = {dot_prod_aff.shape}')\n",
    "        \n",
    "        # square dist attention\n",
    "        Tq = torch.einsum('b p r h a , b r a k -> p h b r k', gq, bbr) + bbt\n",
    "        Tk = -1 * torch.einsum('b p r h a , b r a k -> p h b r k', gk, bbr) + bbt\n",
    "#         print(f'Tq shape = {Tq.shape}')\n",
    "        # dot product\n",
    "        sqr_dist_aff = torch.einsum('p h b i k , p h b j k -> b p h i j k', Tq, Tk)\n",
    "        # norm square\n",
    "        sqr_dist_aff = torch.sum(torch.square(torch.norm(sqr_dist_aff, dim=-1)), dim=1) # b h r r\n",
    "#         print(f'norm_sqr shape = {sqr_dist_aff.shape}')\n",
    "        # multiply head weight\n",
    "        head_w = (F.softplus(self.gamma.repeat(self.heads)) * self.w_c) / 2\n",
    "#         print(f'head_w shape = {head_w.shape}')\n",
    "#         print(f'sqr_dist_aff shape = {sqr_dist_aff.shape}')        \n",
    "        sqr_dist_aff = rearrange(rearrange(sqr_dist_aff, 'b h i j -> b i j h') * head_w, 'b i j h -> b h i j')\n",
    "#         print(f'sqr_dist_aff shape = {sqr_dist_aff.shape}')\n",
    "        \n",
    "        # sum attentions with bias then softmax (B x H x R x R)\n",
    "        attentions = pair_bias + dot_prod_aff + sqr_dist_aff\n",
    "        attentions = torch.softmax(self.w_l * attentions, dim=-1)\n",
    "#         print(f'attentions after softmax shape = {attentions.shape}')\n",
    "        \n",
    "        \n",
    "        # dot with pair values (top) \n",
    "        # B Rq H R x B Rq R C => B R H C\n",
    "        top = torch.einsum('b h i j , b h j d -> b h i d', rearrange(attentions, 'b h i j -> b i h j'), pair_rep) # B H Rq R x B C R R -> B C R R\n",
    "        # concat heads\n",
    "        top = rearrange(top, 'b r h c -> b r (h c)')\n",
    "#         print(f'top shape = {top.shape}')\n",
    "        # transform back to initial dimension\n",
    "        top = self.W_1(top)\n",
    "        \n",
    "        # dot with value points (bot)\n",
    "        # B H Rq Rv x B P Rv H 3 => B R1 H P 3\n",
    "        Tv = torch.einsum('b p r h a , b r a k -> p h b r k', gv, bbr) + bbt\n",
    "#         print(f'Tv shape = {Tv.shape}')\n",
    "        bot = torch.einsum('b h i j , p h b j a -> b i h p a', attentions, Tv)\n",
    "        # invert backbone frames\n",
    "        bbr_inv = torch.linalg.inv(bbr)\n",
    "        # affine transform\n",
    "        bot = torch.einsum('b r h p a , b r a k -> h p b r k', bot, bbr_inv) + bbt\n",
    "        # concat heads\n",
    "        bot = rearrange(bot, 'h p b r a -> b r (h p a)')\n",
    "        # transform back to initial dimension\n",
    "        bot = self.W_2(bot)\n",
    "#         print(f'bot shape = {bot.shape}')\n",
    "        \n",
    "        # dot with matrix v (mid)\n",
    "        out = torch.einsum('b h i j , b h j d -> b h i d', attentions, rv)        \n",
    "        # concat heads\n",
    "        out = rearrange(out, \"b h t d -> b t (h d)\")\n",
    "        # transform back to initial dimension\n",
    "        out = self.W_0(out)\n",
    "        # sum top, mid, bottom\n",
    "        out = out + top\n",
    "#         print(f'output shape = {out.shape}')\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "179349c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 64, 8, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B H Rv Rq x B H Rv P 3\n",
    "a = torch.rand(1,12,64,64)\n",
    "b = torch.rand(1,12,64,8,3)\n",
    "torch.einsum('b h i j , b h i p a -> b h j p a', a, b).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9918e94",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6aa0df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 5\n",
    "R = 64\n",
    "C_m = 128\n",
    "C_z = 64\n",
    "H = 12\n",
    "C = 16\n",
    "N_qp = 4\n",
    "N_pv = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f3bb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_rep = torch.rand(B, R, R, C_z)\n",
    "sing_rep = torch.rand(B, R, C_m)\n",
    "bbr = torch.rand(B, R, 3, 3)\n",
    "bbt = torch.rand(B, R, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8468bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64, 128])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipa = IPA(C_m, C_z, heads=H, dim_head=C)\n",
    "ipa(pair_rep, sing_rep, bbr, bbt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e758f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64, 128])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sing_rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7280a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64, 64, 64])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0bf07959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64, 3, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "447dce1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea776fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
